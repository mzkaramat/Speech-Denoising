{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Denoising Using Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fully connected network with 2 hidden layers and 1000 hidden units each is used for speech denoising.\n",
    "### For the two hidden layers I have used tanh activation functions. (Referred from:- http://paris.cs.illinois.edu/pubs/liu-interspeech2014.pdf)\n",
    "### Apart from this, I have used Xavier initialization for weights, batch normalization and Adam Optimizer.\n",
    "### For the last layer since we are required to output non negative, I have opted for relu activation function.\n",
    "### Applying 20% dropouts during training helped improve the recovered test signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train input and output data\n",
    "s, sr=librosa.load('train_clean_male.wav', sr=None)\n",
    "S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "sn, sr=librosa.load('train_dirty_male.wav', sr=None)\n",
    "X=librosa.stft(sn, n_fft=1024, hop_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposeed the training data to get the data samples in rows and features in columns\n",
    "# Taking the absolute values of the STFT data\n",
    "abs_S = np.abs(S.T)\n",
    "abs_X = np.abs(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 142) (513, 380)\n"
     ]
    }
   ],
   "source": [
    "# Import test data\n",
    "s1, sr=librosa.load('test_x_01.wav', sr=None)\n",
    "S1 =librosa.stft(s1, n_fft=1024, hop_length=512)\n",
    "s2, sr=librosa.load('test_x_02.wav', sr=None)\n",
    "S2 =librosa.stft(s2, n_fft=1024, hop_length=512)\n",
    "print(S1.shape, S2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.abs(S1.T)\n",
    "test2 = np.abs(S2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380, 513)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fully connected network structure\n",
    "num_hidden_1 = 1000 # hidden layer 1\n",
    "num_hidden_2 = 1000 # hidden layer 2\n",
    "num_features = 513 # input features\n",
    "num_output = 513 # number of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', shape=(None,num_features))\n",
    "y = tf.placeholder('float',shape=(None,num_output))\n",
    "keep_probability = tf.placeholder(\"float\")  # probability for dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_net(data, keep_probability):\n",
    "    # Weights and bias initialization\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal(shape=[num_features, num_hidden_1],stddev= tf.sqrt(2/(num_features+num_hidden_1)))),\n",
    "        'w2': tf.Variable(tf.random_normal(shape=[num_hidden_1, num_hidden_2],stddev= tf.sqrt(2/(num_hidden_1+num_hidden_2)))),\n",
    "        'wout': tf.Variable(tf.random_normal(shape=[num_hidden_2, num_output],stddev = tf.sqrt(2/(num_hidden_2 + num_output))))\n",
    "    }\n",
    "    biases = {\n",
    "        'bout': tf.Variable(tf.random_normal([num_output]))\n",
    "    }\n",
    "    # Layer 1 with batch normalization\n",
    "    z1 = data@weights['w1']\n",
    "    batch_mean1, batch_var1 = tf.nn.moments(z1, [0])\n",
    "    z1hat = (z1 - batch_mean1) / tf.sqrt(batch_var1+epsilon)\n",
    "    # Create two new parameters, scale and beta (shift)\n",
    "    scale1 = tf.Variable(tf.ones([num_hidden_1]))\n",
    "    beta1 = tf.Variable(tf.zeros([num_hidden_1]))\n",
    "    z1_hat = tf.nn.batch_normalization(z1, batch_mean1, batch_var1,beta1, scale1,epsilon)\n",
    "    l1 = tf.nn.dropout(tf.nn.tanh(z1_hat), keep_probability)\n",
    "    \n",
    "    # Layer 2 with batch normalization\n",
    "    z2 = l1@weights['w2']\n",
    "    batch_mean2, batch_var2 = tf.nn.moments(z2,[0])\n",
    "    scale2 = tf.Variable(tf.ones([num_hidden_2]))\n",
    "    beta2 = tf.Variable(tf.zeros([num_hidden_2]))\n",
    "    z2_hat = tf.nn.batch_normalization(z2,batch_mean2,batch_var2,beta2,scale2,epsilon)\n",
    "    l2 = tf.nn.dropout(tf.nn.tanh(z2_hat), keep_probability)\n",
    "    \n",
    "    # Output Layer\n",
    "    output = tf.nn.relu(tf.add(tf.matmul(l2, weights['wout']) , biases['bout']))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=250\n",
    "# Small epsilon value for the batch normalization\n",
    "epsilon = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(x):\n",
    "    prediction = fully_connected_net(x, keep_probability)\n",
    "    cost = tf.losses.mean_squared_error(y,prediction, weights=1.0)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(cost)\n",
    "    saver = tf.train.Saver()\n",
    "    #feedforward and backpropogagtion\n",
    "    epochs = 2500\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            start_index = 0\n",
    "            #X_shuffled, S_shuffled = shuffle(abs_X,abs_S)\n",
    "            for _ in range(int(abs_X.shape[0]/batch_size)):\n",
    "                end_index = start_index +batch_size\n",
    "                if end_index > abs_X.shape[0]:\n",
    "                    end_index = abs_X.shape[0]\n",
    "                batch_x = abs_X[start_index:end_index]\n",
    "                batch_y = abs_S[start_index: end_index]\n",
    "                start_index = end_index + 1\n",
    "                _, err = sess.run([train_step, cost], feed_dict={x: batch_x, y: batch_y, keep_probability: 0.8})\n",
    "                epoch_loss += err\n",
    "            for i in range(epoch % 20 == 0):\n",
    "                print('Epoch ',epoch, ' completed out of ',epochs, 'loss: ', epoch_loss)\n",
    "        print('Epoch ',epoch, ' completed out of ',epochs, 'loss: ', epoch_loss)\n",
    "        saver.save(sess, 'model2/')\n",
    "       \n",
    "        test1_pred = sess.run(prediction, feed_dict = {x: test1, keep_probability: 1})\n",
    "        test2_pred = sess.run(prediction, feed_dict = {x: test2, keep_probability: 1})\n",
    "        \n",
    "        return test1_pred, test2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  completed out of  2500 loss:  5.590878009796143\n",
      "Epoch  20  completed out of  2500 loss:  0.20284949243068695\n",
      "Epoch  40  completed out of  2500 loss:  0.15414383076131344\n",
      "Epoch  60  completed out of  2500 loss:  0.1339481808245182\n",
      "Epoch  80  completed out of  2500 loss:  0.11833239905536175\n",
      "Epoch  100  completed out of  2500 loss:  0.11120777484029531\n",
      "Epoch  120  completed out of  2500 loss:  0.10340986307710409\n",
      "Epoch  140  completed out of  2500 loss:  0.10120680648833513\n",
      "Epoch  160  completed out of  2500 loss:  0.09567071683704853\n",
      "Epoch  180  completed out of  2500 loss:  0.09085197700187564\n",
      "Epoch  200  completed out of  2500 loss:  0.08633249159902334\n",
      "Epoch  220  completed out of  2500 loss:  0.08616419322788715\n",
      "Epoch  240  completed out of  2500 loss:  0.08484551962465048\n",
      "Epoch  260  completed out of  2500 loss:  0.08111290307715535\n",
      "Epoch  280  completed out of  2500 loss:  0.08034995244815946\n",
      "Epoch  300  completed out of  2500 loss:  0.07909437408670783\n",
      "Epoch  320  completed out of  2500 loss:  0.07686379877850413\n",
      "Epoch  340  completed out of  2500 loss:  0.07328106742352247\n",
      "Epoch  360  completed out of  2500 loss:  0.07401175936684012\n",
      "Epoch  380  completed out of  2500 loss:  0.0712450984865427\n",
      "Epoch  400  completed out of  2500 loss:  0.07281019305810332\n",
      "Epoch  420  completed out of  2500 loss:  0.0716445199213922\n",
      "Epoch  440  completed out of  2500 loss:  0.06817408511415124\n",
      "Epoch  460  completed out of  2500 loss:  0.06998154055327177\n",
      "Epoch  480  completed out of  2500 loss:  0.07254386600106955\n",
      "Epoch  500  completed out of  2500 loss:  0.06469512172043324\n",
      "Epoch  520  completed out of  2500 loss:  0.06910089356824756\n",
      "Epoch  540  completed out of  2500 loss:  0.06349962437525392\n",
      "Epoch  560  completed out of  2500 loss:  0.06171988043934107\n",
      "Epoch  580  completed out of  2500 loss:  0.06156269460916519\n",
      "Epoch  600  completed out of  2500 loss:  0.06029248470440507\n",
      "Epoch  620  completed out of  2500 loss:  0.0599446757696569\n",
      "Epoch  640  completed out of  2500 loss:  0.061000056099146605\n",
      "Epoch  660  completed out of  2500 loss:  0.05875608278438449\n",
      "Epoch  680  completed out of  2500 loss:  0.05816017463803291\n",
      "Epoch  700  completed out of  2500 loss:  0.05722363945096731\n",
      "Epoch  720  completed out of  2500 loss:  0.061402402352541685\n",
      "Epoch  740  completed out of  2500 loss:  0.05646337103098631\n",
      "Epoch  760  completed out of  2500 loss:  0.05583440372720361\n",
      "Epoch  780  completed out of  2500 loss:  0.05590394511818886\n",
      "Epoch  800  completed out of  2500 loss:  0.053886521607637405\n",
      "Epoch  820  completed out of  2500 loss:  0.05528097110800445\n",
      "Epoch  840  completed out of  2500 loss:  0.05421717744320631\n",
      "Epoch  860  completed out of  2500 loss:  0.05349719384685159\n",
      "Epoch  880  completed out of  2500 loss:  0.05176577693782747\n",
      "Epoch  900  completed out of  2500 loss:  0.053666465217247605\n",
      "Epoch  920  completed out of  2500 loss:  0.051404079888015985\n",
      "Epoch  940  completed out of  2500 loss:  0.0547162382863462\n",
      "Epoch  960  completed out of  2500 loss:  0.04945682408288121\n",
      "Epoch  980  completed out of  2500 loss:  0.05003967462107539\n",
      "Epoch  1000  completed out of  2500 loss:  0.04829246480949223\n",
      "Epoch  1020  completed out of  2500 loss:  0.048717389116063714\n",
      "Epoch  1040  completed out of  2500 loss:  0.04789533745497465\n",
      "Epoch  1060  completed out of  2500 loss:  0.04851696081459522\n",
      "Epoch  1080  completed out of  2500 loss:  0.04766009491868317\n",
      "Epoch  1100  completed out of  2500 loss:  0.04716173838824034\n",
      "Epoch  1120  completed out of  2500 loss:  0.04719536076299846\n",
      "Epoch  1140  completed out of  2500 loss:  0.04664503340609372\n",
      "Epoch  1160  completed out of  2500 loss:  0.04657329572364688\n",
      "Epoch  1180  completed out of  2500 loss:  0.04510459443554282\n",
      "Epoch  1200  completed out of  2500 loss:  0.04476548754610121\n",
      "Epoch  1220  completed out of  2500 loss:  0.04544970020651817\n",
      "Epoch  1240  completed out of  2500 loss:  0.04405565978959203\n",
      "Epoch  1260  completed out of  2500 loss:  0.04386376915499568\n",
      "Epoch  1280  completed out of  2500 loss:  0.04309018375352025\n",
      "Epoch  1300  completed out of  2500 loss:  0.043614109978079796\n",
      "Epoch  1320  completed out of  2500 loss:  0.042319199768826365\n",
      "Epoch  1340  completed out of  2500 loss:  0.04221176728606224\n",
      "Epoch  1360  completed out of  2500 loss:  0.04230695986188948\n",
      "Epoch  1380  completed out of  2500 loss:  0.042794176610186696\n",
      "Epoch  1400  completed out of  2500 loss:  0.041437467094510794\n",
      "Epoch  1420  completed out of  2500 loss:  0.04111619247123599\n",
      "Epoch  1440  completed out of  2500 loss:  0.040888432413339615\n",
      "Epoch  1460  completed out of  2500 loss:  0.0405141047667712\n",
      "Epoch  1480  completed out of  2500 loss:  0.039708610624074936\n",
      "Epoch  1500  completed out of  2500 loss:  0.039520107908174396\n",
      "Epoch  1520  completed out of  2500 loss:  0.04004838992841542\n",
      "Epoch  1540  completed out of  2500 loss:  0.03931441833265126\n",
      "Epoch  1560  completed out of  2500 loss:  0.038952263770624995\n",
      "Epoch  1580  completed out of  2500 loss:  0.03880544169805944\n",
      "Epoch  1600  completed out of  2500 loss:  0.038768226746469736\n",
      "Epoch  1620  completed out of  2500 loss:  0.039605010766536\n",
      "Epoch  1640  completed out of  2500 loss:  0.037858126452192664\n",
      "Epoch  1660  completed out of  2500 loss:  0.03802062710747123\n",
      "Epoch  1680  completed out of  2500 loss:  0.03733134875074029\n",
      "Epoch  1700  completed out of  2500 loss:  0.03764195088297129\n",
      "Epoch  1720  completed out of  2500 loss:  0.03741730540059507\n",
      "Epoch  1740  completed out of  2500 loss:  0.03764360351487994\n",
      "Epoch  1760  completed out of  2500 loss:  0.036471929401159286\n",
      "Epoch  1780  completed out of  2500 loss:  0.03584007220342755\n",
      "Epoch  1800  completed out of  2500 loss:  0.036005018977448344\n",
      "Epoch  1820  completed out of  2500 loss:  0.03614037111401558\n",
      "Epoch  1840  completed out of  2500 loss:  0.035808013984933496\n",
      "Epoch  1860  completed out of  2500 loss:  0.03569248365238309\n",
      "Epoch  1880  completed out of  2500 loss:  0.03557380707934499\n",
      "Epoch  1900  completed out of  2500 loss:  0.03490623598918319\n",
      "Epoch  1920  completed out of  2500 loss:  0.0351645783521235\n",
      "Epoch  1940  completed out of  2500 loss:  0.03479206096380949\n",
      "Epoch  1960  completed out of  2500 loss:  0.034195100888609886\n",
      "Epoch  1980  completed out of  2500 loss:  0.033767710672691464\n",
      "Epoch  2000  completed out of  2500 loss:  0.03381853527389467\n",
      "Epoch  2020  completed out of  2500 loss:  0.03387068072333932\n",
      "Epoch  2040  completed out of  2500 loss:  0.03351849992759526\n",
      "Epoch  2060  completed out of  2500 loss:  0.033610952319577336\n",
      "Epoch  2080  completed out of  2500 loss:  0.033230753848329186\n",
      "Epoch  2100  completed out of  2500 loss:  0.03292213915847242\n",
      "Epoch  2120  completed out of  2500 loss:  0.03285519964993\n",
      "Epoch  2140  completed out of  2500 loss:  0.033667743438854814\n",
      "Epoch  2160  completed out of  2500 loss:  0.032723240088671446\n",
      "Epoch  2180  completed out of  2500 loss:  0.03336763777770102\n",
      "Epoch  2200  completed out of  2500 loss:  0.03238414949737489\n",
      "Epoch  2220  completed out of  2500 loss:  0.03190364921465516\n",
      "Epoch  2240  completed out of  2500 loss:  0.032575106946751475\n",
      "Epoch  2260  completed out of  2500 loss:  0.031960990745574236\n",
      "Epoch  2280  completed out of  2500 loss:  0.03176539111882448\n",
      "Epoch  2300  completed out of  2500 loss:  0.03121722466312349\n",
      "Epoch  2320  completed out of  2500 loss:  0.031587152974680066\n",
      "Epoch  2340  completed out of  2500 loss:  0.03107794839888811\n",
      "Epoch  2360  completed out of  2500 loss:  0.030848232563585043\n",
      "Epoch  2380  completed out of  2500 loss:  0.030865696957334876\n",
      "Epoch  2400  completed out of  2500 loss:  0.03125925804488361\n",
      "Epoch  2420  completed out of  2500 loss:  0.03094985755160451\n",
      "Epoch  2440  completed out of  2500 loss:  0.030844728462398052\n",
      "Epoch  2460  completed out of  2500 loss:  0.030997369904071093\n",
      "Epoch  2480  completed out of  2500 loss:  0.030472496058791876\n",
      "Epoch  2499  completed out of  2500 loss:  0.029686102643609047\n"
     ]
    }
   ],
   "source": [
    "test1_pred, test2_pred = train_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = (S1/test1.T)* test1_pred.T\n",
    "out2 = (S2/test2.T)* test2_pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_recons = librosa.istft(out1, win_length= 1024, hop_length=512)\n",
    "test2_recons = librosa.istft(out2, win_length= 1024, hop_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.output.write_wav('test_s_01_recons.wav', test1_recons, sr)\n",
    "librosa.output.write_wav('test_s_02_recons.wav', test2_recons, sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
